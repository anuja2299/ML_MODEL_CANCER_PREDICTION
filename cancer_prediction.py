# -*- coding: utf-8 -*-
"""cancer_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tvRGhTtwX1P7lC1EUBHn8bwksI3VCYE-
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Loss function
def compute_loss(y, y_hat):
    m = y.shape[0]
    loss = -1/m * (np.dot(y, np.log(y_hat)) + np.dot(1-y, np.log(1-y_hat)))
    return loss

# Gradient descent function
def gradient_descent(X, y, learning_rate, iterations):
    m, n = X.shape
    w = np.zeros(n)
    b = 0
    losses = []

    for i in range(iterations):
        # Compute the model's predictions
        z = np.dot(X, w) + b
        y_hat = sigmoid(z)

        # Compute the gradients
        dw = 1/m * np.dot(X.T, (y_hat - y))
        db = 1/m * np.sum(y_hat - y)

        # Update the parameters
        w -= learning_rate * dw
        b -= learning_rate * db

        # Compute the loss
        loss = compute_loss(y, y_hat)
        losses.append(loss)

        if i % 100 == 0:
            print(f"Iteration {i}: Loss = {loss}")

    return w, b, losses

# Predict function
def predict(X, w, b):
    z = np.dot(X, w) + b
    y_hat = sigmoid(z)
    return y_hat >= 0.5

# Load the dataset
file_path = '/content/drive/MyDrive/Colab Notebooks/agratas/The_Cancer_data_1500_V2.csv'
data = pd.read_csv(file_path)

# Assuming the last column is the label and the first 7 columns are features
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values

# Optionally normalize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
learning_rate = 0.01
iterations = 1000
w, b, losses = gradient_descent(X_train, y_train, learning_rate, iterations)

# Plot the loss over iterations
plt.plot(losses)
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Loss over Iterations')
plt.show()

# Make predictions on the test set
predictions = predict(X_test, w, b)
accuracy = np.mean(predictions == y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

conf_matrix = confusion_matrix(y_test, predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

# Calculate precision, recall, and F1 score
precision = precision_score(y_test, predictions)
recall = recall_score(y_test, predictions)
f1 = f1_score(y_test, predictions)

print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

# Plot precision, recall, and F1 score
metrics = ['Precision', 'Recall', 'F1 Score']
values = [precision, recall, f1]

plt.figure(figsize=(8, 6))
plt.bar(metrics, values, color=['blue', 'orange', 'green'])
plt.ylim(0, 1)
plt.xlabel('Metrics')
plt.ylabel('Scores')
plt.title('Precision, Recall, and F1 Score')
plt.show()

# Function to test new data
def test_new_data(new_data, scaler, w, b):
    # Preprocess the new data (assuming it's a single example)
    new_data = scaler.transform([new_data])
    prediction = predict(new_data, w, b)
    return prediction[0]

# Example of testing new data
new_data = [2.5, 3.1, 1.2, 4.5, 0.8, 1.5, 3.2,0]  # Example new data point with 7 features
is_cancer = test_new_data(new_data, scaler, w, b)
print(f"Does the patient have cancer? {'Yes' if is_cancer else 'No'}")

from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y_test, predictions)
print(conf_matrix)

from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test, predictions)
recall = recall_score(y_test, predictions)
f1 = f1_score(y_test, predictions)

print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

# Assuming we have a 2D feature space for visualization
def plot_decision_boundary(X, y, w, b):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                         np.arange(y_min, y_max, 0.01))
    Z = predict(np.c_[xx.ravel(), yy.ravel()], w, b)
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, alpha=0.8)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='g')
    plt.title("Decision Boundary")
    plt.show()

# Note: This visualization works only for 2D data
if X.shape[1] == 2:
    plot_decision_boundary(X_test, y_test, w, b)